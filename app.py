import streamlit as st
import os
import tempfile
import time
from pathlib import Path
from openai import OpenAI

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==========================================
# âš™ï¸ ì„¤ì • ë° ìƒìˆ˜
# ==========================================
st.set_page_config(page_title="AI íŒŸìºìŠ¤íŠ¸ ë²ˆì—­ê¸°", page_icon="ğŸ™ï¸", layout="wide")

PRICE_WHISPER_PER_MIN = 0.006 
PRICE_GPT_INPUT_1M = 0.15
PRICE_GPT_OUTPUT_1M = 0.60
PRICE_TTS_1M_CHAR = 15.00
EXCHANGE_RATE = 1450 # í™˜ìœ¨ ì•½ê°„ ì¡°ì •

# ==========================================
# ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ì´ì‹)
# ==========================================

def split_audio_file(file_path: Path, chunk_size_mb: int = 24) -> list[str]:
    """25MB ì´ìƒ íŒŒì¼ ë¶„í• """
    chunk_size = chunk_size_mb * 1024 * 1024
    file_size = file_path.stat().st_size
    
    if file_size <= chunk_size:
        return [str(file_path)]

    st.toast(f"âœ‚ï¸ íŒŒì¼ì´ í½ë‹ˆë‹¤({file_size / (1024*1024):.1f}MB). ë¶„í•  ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", icon="âœ‚ï¸")
    chunk_files = []
    
    with open(file_path, 'rb') as f:
        part_num = 0
        while True:
            chunk_data = f.read(chunk_size)
            if not chunk_data:
                break
            
            part_path = file_path.parent / f"{file_path.stem}_part{part_num}{file_path.suffix}"
            with open(part_path, 'wb') as chunk_f:
                chunk_f.write(chunk_data)
            
            chunk_files.append(str(part_path))
            part_num += 1
            
    return chunk_files

def transcribe_with_progress(client, audio_path, model="whisper-1"):
    """STT ìˆ˜í–‰ ë° ì§„í–‰ë¥  í‘œì‹œ"""
    chunk_files = split_audio_file(Path(audio_path), chunk_size_mb=20)
    full_transcript = []
    
    progress_text = "ìŒì„± ì¸ì‹ ì¤‘ (STT)..."
    my_bar = st.progress(0, text=progress_text)

    for idx, chunk_file in enumerate(chunk_files):
        with open(chunk_file, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model=model,
                file=audio_file,
                response_format="text"
            )
            full_transcript.append(transcript)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        percent = int((idx + 1) / len(chunk_files) * 100)
        my_bar.progress(percent, text=f"{progress_text} ({idx+1}/{len(chunk_files)})")

        if len(chunk_files) > 1:
            os.remove(chunk_file)
            
    my_bar.empty()
    return " ".join(full_transcript)

def translate_long_text(text, model="gpt-4o-mini"):
    """ë²ˆì—­ ìˆ˜í–‰"""
    llm = ChatOpenAI(model=model, temperature=0, api_key=os.environ['OPENAI_API_KEY'])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a professional translator. Translate faithfully, preserve formatting. Do not add explanations."),
        ("human", "Translate the following from English to Korean:\n\n{chunk}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    chunks = splitter.split_text(text)
    
    translated_chunks = []
    my_bar = st.progress(0, text="ë²ˆì—­ ì¤‘ (GPT)...")
    
    for i, chunk in enumerate(chunks):
        out = chain.invoke({"chunk": chunk})
        translated_chunks.append(out)
        my_bar.progress(int((i + 1) / len(chunks) * 100))
        
    my_bar.empty()
    return "\n".join(translated_chunks)

def tts_chunked(text, client, model="tts-1", voice="nova"):
    """TTS ìˆ˜í–‰"""
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50, separators=["\n\n", "\n", ". ", " ", ""])
    chunks = splitter.split_text(text)
    
    temp_dir = Path(tempfile.gettempdir()) / "tts_parts"
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    mp3_files = []
    my_bar = st.progress(0, text="ì˜¤ë””ì˜¤ ìƒì„± ì¤‘ (TTS)...")
    
    for i, chunk in enumerate(chunks):
        out_mp3 = temp_dir / f"part_{i:04d}_{int(time.time())}.mp3"
        response = client.audio.speech.create(model=model, voice=voice, input=chunk)
        response.stream_to_file(out_mp3)
        mp3_files.append(str(out_mp3))
        my_bar.progress(int((i + 1) / len(chunks) * 100))
        
    my_bar.empty()
    return mp3_files

def merge_mp3_simple(part_files):
    """MP3 ë³‘í•© (Pure Python)"""
    if not part_files:
        return None
        
    merged_data = b""
    for p in part_files:
        p_path = Path(p)
        if p_path.exists():
            with open(p_path, "rb") as infile:
                merged_data += infile.read()
            # ë³‘í•© í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.remove(p_path)
            except:
                pass
                
    return merged_data

def display_cost_estimation(eng_text, ko_text):
    """ë¹„ìš© ê³„ì‚° ë° UI í‘œì‹œ"""
    word_count = len(eng_text.split())
    est_duration_min = word_count / 150
    stt_cost = est_duration_min * PRICE_WHISPER_PER_MIN
    
    input_tokens = len(eng_text) / 4
    output_tokens = len(ko_text) / 1.5 
    trans_total = ((input_tokens / 1_000_000) * PRICE_GPT_INPUT_1M) + \
                  ((output_tokens / 1_000_000) * PRICE_GPT_OUTPUT_1M)
    
    tts_cost = (len(ko_text) / 1_000_000) * PRICE_TTS_1M_CHAR
    total_usd = stt_cost + trans_total + tts_cost
    total_krw = total_usd * EXCHANGE_RATE
    
    st.markdown("### ğŸ’° ì˜ˆìƒ ìš”ê¸ˆ ëª…ì„¸ì„œ")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("STT (Whisper)", f"${stt_cost:.4f}", f"{est_duration_min:.1f}ë¶„")
    col2.metric("ë²ˆì—­ (GPT)", f"${trans_total:.4f}", f"{int(output_tokens):,} í† í°")
    col3.metric("TTS (Audio)", f"${tts_cost:.4f}", f"{len(ko_text):,} ì")
    col4.metric("ì´ í•©ê³„", f"${total_usd:.4f}", f"ì•½ {int(total_krw):,}ì›")
    
    st.info("â€» ìœ„ ê¸ˆì•¡ì€ ì¶”ì •ì¹˜ì´ë©° ì‹¤ì œ ì²­êµ¬ ê¸ˆì•¡ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ==========================================
# ğŸ–¥ï¸ Streamlit ë©”ì¸ UI
# ==========================================

with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", type="password", help="sk-ë¡œ ì‹œì‘í•˜ëŠ” í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    if api_key:
        os.environ['OPENAI_API_KEY'] = api_key
        
    voice_option = st.selectbox("ì„±ìš° ì„ íƒ (TTS)", ["nova", "alloy", "echo", "fable", "onyx", "shimmer"])
    st.markdown("---")
    st.markdown("**ì‚¬ìš© ëª¨ë¸:**\n- STT: whisper-1\n- ë²ˆì—­: gpt-4o-mini\n- TTS: tts-1")

st.title("ğŸ™ï¸ íŒŸìºìŠ¤íŠ¸ AI ë²ˆì—­ê¸°")
st.markdown("ì˜ì–´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ **í•œê¸€ í…ìŠ¤íŠ¸ë¡œ ë²ˆì—­**í•˜ê³  **í•œêµ­ì–´ ì˜¤ë””ì˜¤**ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("MP3 íŒŒì¼ ì—…ë¡œë“œ", type=["mp3"])

if uploaded_file and api_key:
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    st.audio(uploaded_file, format="audio/mp3")

    if st.button("ğŸš€ ë²ˆì—­ ë° ì˜¤ë””ì˜¤ ìƒì„± ì‹œì‘", type="primary"):
        client = OpenAI(api_key=api_key)
        
        try:
            with st.status("ì‘ì—… ì§„í–‰ ì¤‘...", expanded=True) as status:
                # 1. STT
                st.write("ğŸ”Š ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘ (STT)...")
                eng_text = transcribe_with_progress(client, tmp_file_path)
                st.write("âœ… STT ì™„ë£Œ!")
                
                # 2. ë²ˆì—­
                st.write("ğŸ“ í•œê¸€ë¡œ ë²ˆì—­ ì¤‘ (GPT)...")
                ko_text = translate_long_text(eng_text)
                st.write("âœ… ë²ˆì—­ ì™„ë£Œ!")
                
                # 3. ë¹„ìš© ê³„ì‚°
                display_cost_estimation(eng_text, ko_text)
                
                # 4. TTS
                st.write("ğŸ™ï¸ í•œêµ­ì–´ ì˜¤ë””ì˜¤ ìƒì„± ì¤‘ (TTS)...")
                mp3_parts = tts_chunked(ko_text, client, voice=voice_option)
                
                # 5. ë³‘í•©
                st.write("ğŸ’¿ ì˜¤ë””ì˜¤ ë³‘í•© ì¤‘...")
                final_audio_bytes = merge_mp3_simple(mp3_parts)
                
                status.update(label="ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", state="complete", expanded=False)

            # ê²°ê³¼ í™”ë©´ í‘œì‹œ
            st.divider()
            
            col_txt1, col_txt2 = st.columns(2)
            with col_txt1:
                with st.expander("ì˜ì–´ ì›ë¬¸ (Transcript)"):
                    st.text_area("English", eng_text, height=300)
            with col_txt2:
                with st.expander("í•œê¸€ ë²ˆì—­ (Translation)"):
                    st.text_area("Korean", ko_text, height=300)
                    st.download_button("ğŸ“œ ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ", ko_text, file_name="script_ko.txt")

            st.subheader("ğŸ§ ìµœì¢… ê²°ê³¼ë¬¼")
            st.audio(final_audio_bytes, format="audio/mp3")
            
            st.download_button(
                label="ğŸ“¥ ìµœì¢… MP3 ë‹¤ìš´ë¡œë“œ",
                data=final_audio_bytes,
                file_name="translated_podcast.mp3",
                mime="audio/mp3",
                type="primary"
            )

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        finally:
            # ì›ë³¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)

elif uploaded_file and not api_key:
    st.warning("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")